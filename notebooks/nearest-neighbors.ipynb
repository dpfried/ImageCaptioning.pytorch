{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dfried/projects/ImageCaptioning.pytorch\n"
     ]
    }
   ],
   "source": [
    "cd /home/dfried/projects/ImageCaptioning.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/dfried/projects/ImageCaptioning.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import captioning.utils.opts as opts\n",
    "import captioning.utils.misc as utils\n",
    "import captioning.models as models\n",
    "from captioning.utils import eval_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioning.data.dataloader import DataLoader\n",
    "from captioning.data.dataloaderraw import DataLoaderRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "opts.add_eval_options(parser)\n",
    "opts.add_diversity_opts(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=0, beam_size=1, block_trigrams=0, coco_json='', decoding_constraint=0, diversity_lambda=0.5, dump_images=1, dump_json=1, dump_path=0, eval_oracle=1, group_size=1, id='', image_folder='', image_root='', input_att_dir='', input_box_dir='', input_fc_dir='', input_json='', input_label_h5='', language_eval=0, length_penalty='', max_length=20, num_images=-1, remove_bad_endings=0, sample_method='greedy', sample_n=1, sample_n_method='sample', split='test', suppress_UNK=1, temperature=1.0, verbose_beam=1, verbose_loss=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = 'models/updown/model-best.pth'\n",
    "infos_fname = 'models/updown/infos_tds-best.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab', 'opt', 'best_val_score', 'iter', 'iterators', 'epoch', 'split_ix'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(infos_fname, 'rb') as f:\n",
    "    infos = utils.pickle_load(f)\n",
    "infos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = ['input_fc_dir', 'input_att_dir', 'input_box_dir', 'input_label_h5', 'input_json', 'batch_size', 'id']\n",
    "ignore = ['start_from']\n",
    "\n",
    "for k in vars(infos['opt']).keys():\n",
    "    if k in replace:\n",
    "        setattr(opt, k, getattr(opt, k) or getattr(infos['opt'], k, ''))\n",
    "    elif k not in ignore:\n",
    "        if not k in vars(opt):\n",
    "            vars(opt).update({k: vars(infos['opt'])[k]}) # copy over options from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.vocab = infos['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.setup(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del opt.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpDownModel(\n",
       "  (embed): Sequential(\n",
       "    (0): Embedding(9488, 1000)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc_embed): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (att_embed): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (logit): Linear(in_features=1000, out_features=9488, bias=True)\n",
       "  (ctx2att): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (core): UpDownCore(\n",
       "    (att_lstm): LSTMCell(3000, 1000)\n",
       "    (lang_lstm): LSTMCell(2000, 1000)\n",
       "    (attention): Attention(\n",
       "      (h2att): Linear(in_features=1000, out_features=512, bias=True)\n",
       "      (alpha_net): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname, map_location='cpu'))\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9487\n",
      "DataLoader loading h5 file:  data/cocobu_fc data/cocobu_att data/cocotalk_box data/cocotalk_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(opt, shuffle_override=False, wrap_override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = loader.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE:\n",
    "    ixs = []\n",
    "    ids = []\n",
    "    file_paths = []\n",
    "    features = []\n",
    "    captions = []\n",
    "\n",
    "    loader.reset_iterator(split)\n",
    "\n",
    "    for batch_ix in tqdm.trange(len(loader.loaders[split])):\n",
    "        data = loader.get_batch(split)\n",
    "        infos = data['infos']\n",
    "        ixs.extend([d['ix'] for d in infos])\n",
    "        ids.extend([d['id'] for d in infos])\n",
    "        file_paths.extend([d['file_path'] for d in infos])\n",
    "        features.append(data['fc_feats'])\n",
    "\n",
    "        batch_captions = []\n",
    "        for batch_labels in data['labels']:\n",
    "            instance_captions = []\n",
    "            for img_labels in batch_labels:\n",
    "                caption = [vocab[str(ix.item())] for ix in img_labels if ix != 0]\n",
    "                instance_captions.append(caption)\n",
    "            batch_captions.append(instance_captions)\n",
    "        captions.extend(batch_captions)\n",
    "    features_array = torch.cat(features, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions_from_batch(data):\n",
    "    batch_captions = []\n",
    "    for batch_labels in data['labels']:\n",
    "        instance_captions = []\n",
    "        for img_labels in batch_labels:\n",
    "            caption = [vocab[str(ix.item())] for ix in img_labels if ix != 0]\n",
    "            instance_captions.append(caption)\n",
    "        batch_captions.append(instance_captions)\n",
    "    return batch_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/cocobu_fc/all_{}.pkl'.format(split)\n",
    "if REGENERATE:\n",
    "    with open(fname, 'wb') as f:\n",
    "        d = {\n",
    "            'ixs': ixs,\n",
    "            'ids': ids,\n",
    "            'file_paths': file_paths,\n",
    "            'features': features_array,\n",
    "            'captions': captions,\n",
    "        }\n",
    "        pickle.dump(d, f)\n",
    "else:\n",
    "    with open(fname, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    ixs, ids, file_paths, features_array, captions = d['ixs'], d['ids'], d['file_paths'], d['features'], d['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tag(tag, inner):\n",
    "    return f'<{tag}>{inner}</{tag}>'\n",
    "\n",
    "def image_html(image_path, width=300, border=False):\n",
    "    if border:\n",
    "        style = ' style=\"border: 5px solid #0FF\" '\n",
    "    else:\n",
    "        style = ''\n",
    "    return f'<img width={width} src=\"{image_path}\" {style}></img>'\n",
    "\n",
    "def captions_html(captions):\n",
    "    #return wrap_tag('p', '<br>'.join(' '.join(cap) for cap in captions))\n",
    "    return wrap_tag('ol', ''.join(wrap_tag('li', cap) for cap in captions))\n",
    "\n",
    "def images_html(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    rows = []\n",
    "    for ix in range(0, len(image_paths), num_per_row):\n",
    "        items = [wrap_tag('td', image_html(image_paths[image_ix], width=width, border=image_ix == target)) \n",
    "                 for image_ix in range(ix, ix+num_per_row) if image_ix < len(image_paths)]\n",
    "        rows.append(wrap_tag('tr', ''.join(items)))\n",
    "        if captions is not None:\n",
    "            cap_html = [\n",
    "                wrap_tag('td', captions_html(captions[image_ix]))\n",
    "                for image_ix in range(ix, ix+num_per_row)\n",
    "                if image_ix < len(image_paths)\n",
    "            ]\n",
    "            rows.append(wrap_tag('tr', ''.join(cap_html)))\n",
    "    return wrap_tag('table', ''.join(rows))\n",
    "\n",
    "def display_images(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    display(HTML(images_html(image_paths, width=width, num_per_row=num_per_row, target=target, captions=captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_batch(img_fc_feat, k, include_self=False, self_ix=None):\n",
    "    assert len(img_fc_feat.shape) == 1, \"should be the features for a single image\"\n",
    "    D, I = index.search(img_fc_feat[None], k)\n",
    "    n_images, k_ = D.shape\n",
    "    assert n_images == 1\n",
    "    indices = []\n",
    "    if include_self:\n",
    "        assert self_ix is not None\n",
    "        indices.append(self_ix)\n",
    "    indices.extend([ixs[i] for i in I.flatten()])\n",
    "    data = [loader.dataset[ix, 0, False] for ix in indices]\n",
    "    batch = loader.dataset.collate_func(data, 'train')\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_neighbors(features, k=5, num_per_row=5):\n",
    "    neighbor_batch = get_neighbor_batch(features.flatten(), k)\n",
    "    paths_k = [d['file_path'] for d in neighbor_batch['infos']]\n",
    "    captions_k = [[' '.join(c) for c in cs] for cs in get_captions_from_batch(neighbor_batch)]\n",
    "    display_images(paths_k, captions=captions_k, num_per_row=num_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000318219.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000554625.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000445810.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000407121.jpg\" ></img></td></tr><tr><td><ol><li>a young boy standing in front of a computer keyboard</li><li>a little boy wearing headphones and looking at a computer monitor</li><li>he is listening intently to the computer at school</li><li>a young boy stares up at the computer monitor</li><li>a young kid with head phones on using a computer</li></ol></td><td><ol><li>a boy wearing headphones using one computer in a long row of computers</li><li>a little boy with earphones on listening to something</li><li>a group of people sitting at desk using computers</li><li>children sitting at computer stations on a long table</li><li>a small child wearing headphones plays on the computer</li></ol></td><td><ol><li>some small children playing on laptop games excited</li><li>two children work at a desk on laptop computers</li><li>two young ladies work on laptops on a white counter top</li><li>a little girl sitting in front of a laptop computer</li><li>two girls who are sitting in front of laptops</li></ol></td><td><ol><li>a man with glasses sitting at a desktop computer</li><li>two men at a computer playing game with headphones on</li><li>two men are wearing headphones and playing a computer game</li><li>two men are in the dark by a laptop computer</li><li>dark haired man playing a video game on computer</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000412657.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000356922.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000156471.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000467931.jpg\" ></img></td></tr><tr><td><ol><li>a man with a hoodie and headphones on in front of a computer</li><li>a person sitting in front of a laptop computer wearing glasses</li><li>a man with headphones sitting at a desk looking at a computer</li><li>a young man in a red sweatshirt is on the computer</li><li>a man is sitting at the computer desk with a laptop on it</li></ol></td><td><ol><li>people at a work bench table with laptops and other electronic equipment</li><li>two people on computers amongst a table full of debris</li><li>three people are working on two laptops</li><li>hands are at work at a table repairing laptop computers</li><li>a group of people sitting around a pair of laptops</li></ol></td><td><ol><li>a man holding a smart phone while standing next to a credit card reader</li><li>a man looking at something in his hands</li><li>a young man is at a workstation with a phone</li><li>an image of man that is looking at his cellphone</li><li>a young man is using a cell phone near electronics</li></ol></td><td><ol><li>two men sitting around a laptop looking at the screen</li><li>a man at a laptop with another looking on at his screen</li><li>two men stare intently at a computer screen while one works at the keyboard</li><li>two men at a desk working with a laptop computer</li><li>two people looking at a laptop on a desk</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_neighbors(features_array[1], k=8, num_per_row=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_pred_captions = []\n",
    "# for sent in seq:\n",
    "#     this_pred_captions.append([\n",
    "#         [model.vocab.get(str(ix.item()), 'IX_{}'.format(ix.item())) for ix in sent if ix.item() != 0]\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_scores(fc_feats, att_feats, att_masks, seq, add_bos=True):\n",
    "    if add_bos:\n",
    "        seq = torch.cat([torch.zeros(seq.size(0), 1).long().to(seq), seq], 1)\n",
    "    with torch.no_grad():\n",
    "        scores = model(fc_feats, att_feats, seq, att_masks)\n",
    "    mask = (seq[:,:-1] > 0) | (seq[:,1:] > 0)\n",
    "    # TODO: does this include the EOS score?\n",
    "    # seq_t: words input at each position, with 0 for pad\n",
    "    # seq_t: 0, w_0, w_1, w_2, ..., w_k, 0, ...\n",
    "    # mask : 1, 1  , 1  , 1  , 1  , 1  , 0, ...\n",
    "    # selected_scores: w_0 + ... + w_k\n",
    "#     return scores[:,:-1].gather(2, seq[:,1:].unsqueeze(2)).squeeze(2)\n",
    "    selected_scores = (scores[:,:-1].gather(2, seq[:,1:].unsqueeze(2)).squeeze(2) * mask)\n",
    "    return selected_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_product_scores(fc_feats, att_feats, att_masks, seq, add_bos=True):\n",
    "    n_captions = seq.size(0)\n",
    "    n_images = fc_feats.size(0)\n",
    "    assert att_feats.size(0) == n_images\n",
    "    assert att_masks.size(0) == n_images\n",
    "    fc_feats_t = fc_feats.unsqueeze(0).repeat_interleave(n_captions, dim=0)\n",
    "    att_feats_t = att_feats.unsqueeze(0).repeat_interleave(n_captions, dim=0)\n",
    "    att_masks_t = att_masks.unsqueeze(0).repeat_interleave(n_captions, dim=0)\n",
    "    \n",
    "    fc_feats_t = einops.rearrange(fc_feats_t, 'caps imgs d -> (caps imgs) d')\n",
    "    att_feats_t = einops.rearrange(att_feats_t, 'caps imgs obj d -> (caps imgs) obj d')\n",
    "    att_masks_t = einops.rearrange(att_masks_t, 'caps imgs obj -> (caps imgs) obj')\n",
    "    seq_t = seq.unsqueeze(1).repeat_interleave(n_images, dim=1)\n",
    "    seq_t = einops.rearrange(seq_t, 'caps imgs d -> (caps imgs) d')\n",
    "    \n",
    "    scores_per_timestep = caption_scores(fc_feats_t, att_feats_t, att_masks_t, seq_t, add_bos=add_bos)\n",
    "    scores = scores_per_timestep.sum(1)\n",
    "    scores = einops.rearrange(scores, '(caps imgs) -> caps imgs', caps=n_captions, imgs=n_images)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.reset_iterator('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.get_batch('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_feats = data['fc_feats']\n",
    "this_ixs = [d['ix'] for d in data['infos']]\n",
    "this_ids = [d['id'] for d in data['infos']]\n",
    "this_paths = [d['file_path'] for d in data['infos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_trace()\n",
    "sample_n = 10\n",
    "input_data = data['fc_feats'].cuda(), data['att_feats'].cuda(), data['att_masks'].cuda(), data\n",
    "n_predictions = []\n",
    "eval_kwargs = {\n",
    "    'sample_n_method': 'bs',\n",
    "    'sample_n': sample_n,\n",
    "    'temperature': 0.25,\n",
    "    'verbose': False,\n",
    "}\n",
    "eval_utils.eval_split_n(model, n_predictions, input_data=input_data, eval_kwargs=eval_kwargs)\n",
    "captions_by_id = {}\n",
    "log_prob_by_id = {}\n",
    "seq_by_id = {}\n",
    "for k, ds in groupby(n_predictions, lambda d: d['image_id']):\n",
    "    ds = list(ds)\n",
    "    captions_by_id[k] = [d['caption'] for d in ds]\n",
    "    log_prob_by_id[k] = [d['log_prob'] for d in ds]\n",
    "    seq_by_id[k] = [d['seq'] for d in ds]\n",
    "this_captions = [\n",
    "    captions_by_id[id_]  for id_ in this_ids\n",
    "]\n",
    "this_log_probs = [\n",
    "    log_prob_by_id[id_] for id_ in this_ids\n",
    "]\n",
    "this_seq = [\n",
    "    torch.stack(seq_by_id[id_], 0) for id_ in this_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000009426.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000009426.jpg\" ></img></td></tr><tr><td><ol><li>a red and white airplane flying in the sky  -5.3846</li><li>a red and white plane flying in the sky  -5.6425</li><li>a red and white airplane flying in the air  -6.2955</li><li>a red and white airplane flying through a blue sky  -6.4906</li><li>a red and white airplane flying in a blue sky  -6.7558</li><li>a red and white plane flying in a blue sky  -6.8799</li><li>the red and white airplane is flying in the sky  -7.9610</li><li>there is a red and white plane flying in the sky  -8.8055</li><li>an airplane flying in the sky with a red tail  -12.0166</li><li>an airplane is flying in the sky with a red tail  -12.4322</li></ol></td><td><ol><li>a red and white airplane flying in a blue sky  -2.2066</li><li>a red and white airplane flying in the sky  -2.2307</li><li>there is a red and white plane flying in the sky  -2.2577</li><li>the red and white airplane is flying in the sky  -2.2766</li><li>an airplane flying in the sky with a red tail  -2.2893</li><li>an airplane is flying in the sky with a red tail  -2.3098</li><li>a red and white plane flying in a blue sky  -2.3132</li><li>a red and white airplane flying in the air  -2.3209</li><li>a red and white airplane flying through a blue sky  -2.4151</li><li>a red and white plane flying in the sky  -2.4287</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000512438.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000183500.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000440786.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000331738.jpg\" ></img></td></tr><tr><td><ol><li>a white and black plane flying with sky in background</li><li>a small passenger plane flying on a sunny day</li><li>a propeller airplane flying under a cloudy blue sky</li><li>looking under an airplane as it flies in the air</li><li>the airplane is flying near the clouds in the sky</li></ol></td><td><ol><li>a person is flying a biplane in the sky</li><li>an old biplane that is flying low for the crowd</li><li>a red and white plane flying through a cloudy sky</li><li>a propeller plane that is flying in the sky</li><li>white and red biplane flying through the air</li></ol></td><td><ol><li>a bed and white propeller plane flying through a blue sky</li><li>a plane in the middle of the air its a propeller plane</li><li>the old plane has been recently painted in red and white</li><li>a very small red and white air plane</li><li>a plane flying in the air on a clear day</li></ol></td><td><ol><li>a yellow and blue biplane flying through the ski</li><li>a small single engine plane in flying in the sky</li><li>a blue red and white airplane is flying</li><li>a small biplane flies through the blue sky</li><li>an old airplane flies through the sky on a nice day</li></ol></td></tr><tr><td><img width=300 src=\"val2014/COCO_val2014_000000295765.jpg\" ></img></td></tr><tr><td><ol><li>an airplane flying through the air on a clear day</li><li>the airplane is flying high in the clear blue sky</li><li>a white airplane with a single propeller and double stacked wings</li><li>a red white and blue airplane flying in the sky</li><li>a small aircraft flying low on a clear sky</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000242139.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000242139.jpg\" ></img></td></tr><tr><td><ol><li>a city street with lots of tall buildings  -7.8097</li><li>a city street with a large building in the background  -10.1655</li><li>a city street with a tall building in the background  -10.6663</li><li>a city street with a large city in the background  -10.7126</li><li>a city street with tall buildings and a large city  -11.6744</li><li>a city street with cars and a tall building  -12.1900</li><li>a city street with tall buildings and a large building  -12.2099</li><li>a city street with tall buildings and a large building in the background  -12.6306</li><li>a city street with cars and people walking on the sidewalk  -13.4368</li><li>a city street with a tall building and a large city  -15.2348</li></ol></td><td><ol><li>a city street with tall buildings and a large building in the background  -1.9513</li><li>a city street with a tall building and a large city  -1.9794</li><li>a city street with tall buildings and a large city  -2.0439</li><li>a city street with tall buildings and a large building  -2.1185</li><li>a city street with a large city in the background  -2.2861</li><li>a city street with a tall building in the background  -2.4150</li><li>a city street with a large building in the background  -2.5029</li><li>a city street with lots of tall buildings  -2.5392</li><li>a city street with cars and a tall building  -2.5657</li><li>a city street with cars and people walking on the sidewalk  -3.1856</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000533739.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000219750.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000230615.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000219225.jpg\" ></img></td></tr><tr><td><ol><li>heavy traffic in a city with a UNK bank building</li><li>a busy downtown street is filled with cars waiting to move</li><li>a green light is shown on this busy multi lane street</li><li>there are many tall buildings and cars in this city</li><li>a large city with UNK buildings and a green light</li></ol></td><td><ol><li>a red and yellow double decker bus on street next to trees</li><li>a double deck tour bus riding down a street through a traffic light</li><li>a large red bus driving down the street</li><li>a double deck bus that is driving down the road</li><li>a red double decker bus driving down a street</li></ol></td><td><ol><li>a man riding down the street in a horse and carriage</li><li>a group of cars parked in a lot</li><li>a horse drawn carriage comes down the street on a clear day</li><li>a horse drawn carriage on a city street</li><li>the cars are sharing the busy road with the horse</li></ol></td><td><ol><li>traffic and people are standing in the downtown area</li><li>a busy 2 way downtown intersection in the city</li><li>there is a very tall tower with a clock on it on this street</li><li>a busy semi busy street with three yellow taxis going down it</li><li>a tall clock tower on a city street near buildings</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000263826.jpg\" ></img></td></tr><tr><td><ol><li>a transit bus moves through a crowded street</li><li>the buses are lined up on the busy street</li><li>view of down town in a city and traffic driving on the opposite side of the</li><li>two public transit buses on a city street</li><li>a picture of an outdoor area that seems great</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000143931.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000143931.jpg\" ></img></td></tr><tr><td><ol><li>a bus with a advertisement on the side of it  -9.9457</li><li>a truck with a mural on the side of it  -10.0262</li><li>a bus with a mural on the side of it  -10.1005</li><li>a blue truck with a advertisement on the side of it  -10.2522</li><li>a blue bus with a advertisement on the side of it  -10.5892</li><li>a blue truck with a sign on the side of it  -11.0073</li><li>a bus with a large advertisement on the side of it  -11.2638</li><li>a blue truck with a blue and white bus  -14.9023</li><li>a blue truck with a blue and white bus on the side  -16.5830</li><li>a blue truck with a blue and white bus on the side of it  -17.4545</li></ol></td><td><ol><li>a blue truck with a advertisement on the side of it  -1.9789</li><li>a blue truck with a blue and white bus on the side of it  -2.0041</li><li>a blue truck with a blue and white bus on the side  -2.0087</li><li>a blue truck with a blue and white bus  -2.1525</li><li>a truck with a mural on the side of it  -2.2120</li><li>a blue truck with a sign on the side of it  -2.2145</li><li>a blue bus with a advertisement on the side of it  -2.3565</li><li>a bus with a mural on the side of it  -2.7477</li><li>a bus with a large advertisement on the side of it  -2.9925</li><li>a bus with a advertisement on the side of it  -3.0165</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000550874.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000381759.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000048196.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000382854.jpg\" ></img></td></tr><tr><td><ol><li>a bus that is sitting in the street with its door open</li><li>a bus traveling down the street in a city</li><li>the political tour bus serves as home base during the UNK</li><li>a bus decorated for a presidential political campaign</li><li>a bus for a politician driving down the street</li></ol></td><td><ol><li>a large truck driving down a busy street filled with traffic</li><li>a refrigerated semi truck drives on a rode beside a smaller car</li><li>the semi truck and car turn the corner close to each other</li><li>small car rides behind a large semi truck with a large bed</li><li>a car and truck are navigating a turn together</li></ol></td><td><ol><li>a long yellow bus advertising a musical play</li><li>a yellow bus with a lion king ad on it</li><li>a large yellow bus with pictures of people in lion costumes and the words the lion</li><li>a bus is covered in an advertisement for a broadway show</li><li>a bus with advertisement painted on the side</li></ol></td><td><ol><li>a black bus on street with flags and buildings in background</li><li>a black bus driving down the road in the middle of the city</li><li>the team bus is parked at the building</li><li>the large tour bus is painted with a dog mascot</li><li>a large black truck driving down a city street</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000514801.jpg\" ></img></td></tr><tr><td><ol><li>a vehicle pulls up next to a building</li><li>the large blue truck is parked at the curb</li><li>a truck that is driving on the road</li><li>a large blue tow truck sitting on the side of a road</li><li>a truck is on the city street</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000040102.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000040102.jpg\" ></img></td></tr><tr><td><ol><li>a couple of giraffe standing next to each other  -5.7969</li><li>a couple of giraffe standing on top of a lush green field  -6.3289</li><li>a couple of giraffes are standing in a field  -6.6013</li><li>a couple of giraffe standing next to each other on a field  -6.6032</li><li>two giraffes are standing in a grassy field  -6.6232</li><li>two giraffes standing in the grass near trees  -7.4514</li><li>two giraffes standing in a field of grass  -8.3326</li><li>two giraffes standing in a grassy field next to trees  -8.4241</li><li>two giraffes standing in a field with trees  -8.5482</li><li>two giraffes standing in a grassy field with trees  -8.6169</li></ol></td><td><ol><li>two giraffes standing in a grassy field with trees  -1.8652</li><li>two giraffes standing in the grass near trees  -1.9938</li><li>two giraffes are standing in a grassy field  -2.0416</li><li>two giraffes standing in a grassy field next to trees  -2.0757</li><li>two giraffes standing in a field of grass  -2.2234</li><li>two giraffes standing in a field with trees  -2.2874</li><li>a couple of giraffe standing on top of a lush green field  -2.4703</li><li>a couple of giraffes are standing in a field  -2.8429</li><li>a couple of giraffe standing next to each other  -2.9213</li><li>a couple of giraffe standing next to each other on a field  -3.0531</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000238713.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000022257.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000508969.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000172426.jpg\" ></img></td></tr><tr><td><ol><li>a group of zebras on a grassy area next to trees</li><li>a group of giraffes stand next in a field by a tree</li><li>a group of giraffes in a field near trees</li><li>a group of four giraffes standing next to each other</li><li>a herd of giraffe walking across a field</li></ol></td><td><ol><li>a herd of giraffe standing next to each other near a forested hillside</li><li>a group of giraffes are standing together on the UNK</li><li>an image of a herd of giraffes in a plain</li><li>half a dozen healthy giraffes hanging out in a field</li><li>a group of giraffes faces and stare in the same direction while one of the faces</li></ol></td><td><ol><li>the giraffes stood together next to the bush</li><li>two giraffes standing in open field with trees</li><li>two giraffes are heading towards trees for leaves</li><li>one giraffe is behind another giraffe on the grass</li><li>two giraffes stand right next to each other</li></ol></td><td><ol><li>two giraffes walking through a spacious grassy field</li><li>two giraffes that are walking together in a field</li><li>two giraffes next to one another near a rock</li><li>two giraffes standing next to each other under a group of trees</li><li>two giraffes standing on all fours next to one another with grass bushes and trees around</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000257983.jpg\" ></img></td></tr><tr><td><ol><li>several giraffes are standing on the short grass</li><li>group of giraffes standing in grass lands</li><li>adult giraffe surrounded by three younger giraffe in the wild</li><li>the three giraffes are standing in the grassy field together</li><li>some giraffes in a field with trees in the background</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000403020.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000403020.jpg\" ></img></td></tr><tr><td><ol><li>a giraffe is laying down in the dirt  -5.5802</li><li>the giraffe is laying down in the dirt  -6.8027</li><li>a giraffe laying on the ground next to a building  -7.1025</li><li>a giraffe laying down in the dirt near a building  -7.6570</li><li>a giraffe laying down in the dirt next to a building  -8.2647</li><li>a giraffe laying on the ground in a zoo  -8.3065</li><li>a giraffe laying on the ground in a dirt  -8.6407</li><li>a giraffe laying down in the dirt in a zoo  -8.9004</li><li>a giraffe laying down on the ground in a zoo  -9.0528</li><li>a giraffe laying down on the ground in a dirt  -9.4864</li></ol></td><td><ol><li>a giraffe laying down in the dirt near a building  -2.1925</li><li>a giraffe laying down in the dirt next to a building  -2.1933</li><li>a giraffe laying on the ground next to a building  -2.1935</li><li>a giraffe laying down on the ground in a dirt  -2.3129</li><li>the giraffe is laying down in the dirt  -2.3140</li><li>a giraffe laying down on the ground in a zoo  -2.3565</li><li>a giraffe laying on the ground in a dirt  -2.3627</li><li>a giraffe laying down in the dirt in a zoo  -2.3633</li><li>a giraffe laying on the ground in a zoo  -2.3737</li><li>a giraffe is laying down in the dirt  -2.3938</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000050101.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000334957.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000067572.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000476871.jpg\" ></img></td></tr><tr><td><ol><li>four giraffes out in a field of some sort</li><li>three large giraffes with one small standing next to a building</li><li>a group of giraffes in a large enclosure next to a stone barn</li><li>three adult and one baby giraffe standing outside</li><li>a stone barn at a zoo with four giraffe standing around</li></ol></td><td><ol><li>a giraffe kneeling down on the ground next to a tree</li><li>a giraffe on its knees in the sand</li><li>a giraffe whose knees a UNK beneath itself</li><li>a giraffe kneeling down on its front legs on sand in a fenced in area</li><li>a giraffe in a fenced area down on its knees</li></ol></td><td><ol><li>several giraffes are on display in a zoo exhibit</li><li>four giraffes standing and lounging in an enclosure</li><li>the giraffes are standing in the sand beside a fence</li><li>four giraffes standing and sitting in an enclosure</li><li>a herd of giraffe standing around an enclosure at a zoo</li></ol></td><td><ol><li>giraffes standing in a dirt lot near a pool of water</li><li>giraffes in a zoo enclosure with a stone wall</li><li>two giraffes and a rhino in an enclosure</li><li>two giraffes look over a fence in a zoo</li><li>giraffes in an enclosure stand together by the water</li></ol></td></tr><tr><td><img width=300 src=\"val2014/COCO_val2014_000000196650.jpg\" ></img></td></tr><tr><td><ol><li>a giraffe standing next to another animal in a field</li><li>a giraffe and a deer standing near a ravine</li><li>a giraffe doing an odd pose in a field in front of a forest</li><li>a giraffe with its back legs spread while it leans forward</li><li>a giraffe is posing for the camera</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000505440.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000505440.jpg\" ></img></td></tr><tr><td><ol><li>a couple of giraffe standing next to each other  -6.3707</li><li>a couple of giraffe standing on top of a lush green field  -6.4391</li><li>a couple of giraffes are standing in a field  -7.0572</li><li>a couple of giraffe standing next to each other on a lush green field  -7.4226</li><li>two giraffes standing in the grass near trees  -8.1003</li><li>two giraffes standing in a grassy area next to trees  -8.6406</li><li>two giraffes standing in a field next to trees  -9.1155</li><li>two giraffes standing in a field with trees  -9.2221</li><li>two giraffes standing in a grassy field next to trees  -9.4280</li><li>two giraffes standing in a field of grass and trees  -9.8691</li></ol></td><td><ol><li>two giraffes standing in a grassy area next to trees  -1.8909</li><li>two giraffes standing in the grass near trees  -1.9922</li><li>two giraffes standing in a field of grass and trees  -2.0097</li><li>two giraffes standing in a field with trees  -2.1959</li><li>two giraffes standing in a grassy field next to trees  -2.1976</li><li>two giraffes standing in a field next to trees  -2.2443</li><li>a couple of giraffe standing on top of a lush green field  -2.4730</li><li>a couple of giraffe standing next to each other on a lush green field  -2.6140</li><li>a couple of giraffes are standing in a field  -2.8668</li><li>a couple of giraffe standing next to each other  -3.3280</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000340345.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000359189.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000119529.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000444464.jpg\" ></img></td></tr><tr><td><ol><li>a group of giraffe standing next to each other in an open field</li><li>a herd of giraffe standing around a large tree stump</li><li>a group of giraffes are nibbling on a large tree trunk</li><li>group of giraffes outside standing around a stump</li><li>a group of giraffes standing around a short UNK tree</li></ol></td><td><ol><li>three giraffes are standing together surrounded by trees and shrubbery</li><li>a group of three giraffe in the wilderness</li><li>a group of giraffes foraging among the grass</li><li>three giraffes gathered together in their own habitat</li><li>giraffes standing next to each other near a forest</li></ol></td><td><ol><li>three giraffes standing in grass with their heads in a tree</li><li>a group of giraffes standing in front of a tree</li><li>three giraffes who are eating from a large tree</li><li>the three giraffes are standing by the tree</li><li>a family of three giraffes is standing under a big tree</li></ol></td><td><ol><li>a group of giraffe standing on top of a field</li><li>one adult giraffe and two kid giraffes standing in the woods</li><li>giraffes in the wild under trees on a sunny day</li><li>the adult giraffe is in the field feeding with the two offspring</li><li>three giraffes standing in the grass among trees and bushes</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000024095.jpg\" ></img></td></tr><tr><td><ol><li>two giraffe in a wooded area with an orange fence</li><li>two giraffes standing on rocks in the middle of a field</li><li>two giraffes in a wooded and grassy area</li><li>two giraffes standing in a green shady field</li><li>two giraffes standing next to each other in front of trees</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000029913.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000029913.jpg\" ></img></td></tr><tr><td><ol><li>a fire hydrant with a hose attached to it  -7.4004</li><li>a fire hydrant is on the side of the road  -8.1863</li><li>a fire hydrant that is on the side of the road  -8.7289</li><li>a fire hydrant that is on the side of a road  -9.0019</li><li>the fire hydrant is on the side of the road  -9.0924</li><li>a fire hydrant is painted white and blue  -10.1041</li><li>a fire hydrant is painted white and black  -10.1185</li><li>a fire hydrant is on the sidewalk next to a car  -10.4407</li><li>a fire hydrant is on a sidewalk next to a car  -10.7089</li><li>a fire hydrant that has been painted white and blue  -11.2274</li></ol></td><td><ol><li>a fire hydrant is on the sidewalk next to a car  -1.4475</li><li>a fire hydrant is on a sidewalk next to a car  -1.4559</li><li>a fire hydrant with a hose attached to it  -1.5761</li><li>a fire hydrant is painted white and black  -2.6243</li><li>a fire hydrant that has been painted white and blue  -2.8812</li><li>a fire hydrant is on the side of the road  -2.9437</li><li>the fire hydrant is on the side of the road  -3.1696</li><li>a fire hydrant that is on the side of a road  -3.2031</li><li>a fire hydrant that is on the side of the road  -3.2978</li><li>a fire hydrant is painted white and blue  -3.7296</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000328662.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000490887.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000016805.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000236769.jpg\" ></img></td></tr><tr><td><ol><li>a motorcycle parked in a parking lot next to a car</li><li>an antique indian motorcycle is parked next to the sidewalk</li><li>motorcycle parked on the edge of a street</li><li>an old indian motorcycle parked at the curb of a street</li><li>a motorcycle parked on a sidewalk next to a street</li></ol></td><td><ol><li>a motorcycle parked next to a sidewalk on the street</li><li>the motorcycle is parked at the curb near the bicycles</li><li>a street scene with the motorcycle and bicycles on the side of the road</li><li>bicycles and a motorcycle parked on a city sidewalk</li><li>a motorcycle and bicycles parked on a city street</li></ol></td><td><ol><li>the yellow fire hydrant is on the curb as cars pass by</li><li>a yellow fire hydrant sitting on the side of a road</li><li>a yellow fire hydrant next to a street</li><li>a yellow fire hydrant that is on a sidewalk</li><li>a fire hydrant sits next to a city street</li></ol></td><td><ol><li>a red white and blue fire hydrant covered in stars</li><li>the fire hydrant is painted red white and blue</li><li>a close up of a fire hydrant UNK red white and blue with stars</li><li>a fire hydrant painted red white and blue are on the curb</li><li>a fire hydrant painted red white and blue with white stars</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000517835.jpg\" ></img></td></tr><tr><td><ol><li>a hydrant that is sitting on the sidewalk</li><li>a fire hydrant is next to a cone on a sidewalk</li><li>a pipe sticking out of a paved surface next to a street grate</li><li>there is a water hole on the street</li><li>there is construction work being done on an urban street</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000431573.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000431573.jpg\" ></img></td></tr><tr><td><ol><li>a red fire hydrant sitting in the middle of a sidewalk  -7.0782</li><li>a red fire hydrant in front of a tree  -7.2148</li><li>a red fire hydrant in the middle of a park  -7.5111</li><li>a fire hydrant in the middle of a park  -7.6126</li><li>a red fire hydrant in the middle of a sidewalk  -7.9263</li><li>a fire hydrant in the middle of a sidewalk  -8.0315</li><li>a fire hydrant is spraying water onto a street  -8.3755</li><li>a red fire hydrant is in the middle of a sidewalk  -9.2053</li><li>a red fire hydrant in a city street  -9.6061</li><li>a red fire hydrant in a park next to a tree  -10.1319</li></ol></td><td><ol><li>a red fire hydrant in a park next to a tree  -2.0400</li><li>a red fire hydrant in a city street  -2.0684</li><li>a red fire hydrant in front of a tree  -2.0851</li><li>a red fire hydrant is in the middle of a sidewalk  -2.0884</li><li>a red fire hydrant in the middle of a park  -2.0914</li><li>a red fire hydrant in the middle of a sidewalk  -2.1153</li><li>a red fire hydrant sitting in the middle of a sidewalk  -2.1219</li><li>a fire hydrant in the middle of a park  -2.7492</li><li>a fire hydrant in the middle of a sidewalk  -3.1231</li><li>a fire hydrant is spraying water onto a street  -3.7515</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000430144.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000148318.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000127455.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000522015.jpg\" ></img></td></tr><tr><td><ol><li>a blue and white fire hydrant on the sidewalk</li><li>a blue and white fire hydrant on the side of the street</li><li>a blue fire hydrant with a white top sits beside a road</li><li>a blue fire hydrant sits in the middle of a sidewalk</li><li>a blue and white fire hydrant sitting on top of a sidewalk</li></ol></td><td><ol><li>a fire hydrant in a city with water pouring out of both sides</li><li>a fire hydrant has water streaming out of two holes on its side</li><li>a green fire hydrant pouring water from two of its spouts</li><li>a fire hydrant that is open with water coming out of two holes</li><li>a fire hydrant with water pouring out of it</li></ol></td><td><ol><li>a fire hydrant next to a bush at a park</li><li>a parking meter on the side of a wooded street</li><li>a fire hydrant on a neighborhood street with trees and shrubs around it</li><li>a street corner with a blue fire hydrant</li><li>a scenic view of a wooded area with parking meter</li></ol></td><td><ol><li>a blue and pink fire hydrant spewing out water onto a street</li><li>a fire hydrant open spilling water onto the street</li><li>a pink faded fire hydrant with dirty water coming out of it</li><li>a fire hydrant is open with water coming out</li><li>open fire hydrant with warning cone in urban city setting</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000536884.jpg\" ></img></td></tr><tr><td><ol><li>a fire hydrant on the corner of a neighborhood street</li><li>a fire hydrant on the corner of a street</li><li>a yellow and green fire hydrant sitting on the side of a road</li><li>the fire hydrant is green and yellow</li><li>a fire hydrant sitting near a sign beside the street</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000332654.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000332654.jpg\" ></img></td></tr><tr><td><ol><li>a yellow and blue fire hydrant sitting on the side of a road  -4.9590</li><li>a yellow fire hydrant sitting on the side of a road  -5.1579</li><li>a blue and yellow fire hydrant sitting on the side of a road  -5.4978</li><li>a yellow and blue fire hydrant on a sidewalk  -5.8938</li><li>a blue and yellow fire hydrant on a sidewalk  -6.5236</li><li>the fire hydrant is on the side of the road  -8.2083</li><li>a fire hydrant on a sidewalk next to a street  -8.2193</li><li>a yellow fire hydrant on a sidewalk next to a street  -8.2990</li><li>a fire hydrant on a sidewalk near a street  -8.3000</li><li>a yellow fire hydrant on a sidewalk near a street  -8.9389</li></ol></td><td><ol><li>a blue and yellow fire hydrant sitting on the side of a road  -1.7379</li><li>a blue and yellow fire hydrant on a sidewalk  -1.9939</li><li>a yellow and blue fire hydrant sitting on the side of a road  -2.1876</li><li>a yellow fire hydrant sitting on the side of a road  -2.2820</li><li>a yellow fire hydrant on a sidewalk near a street  -2.4456</li><li>a yellow fire hydrant on a sidewalk next to a street  -2.4725</li><li>a yellow and blue fire hydrant on a sidewalk  -2.5455</li><li>a fire hydrant on a sidewalk next to a street  -2.5564</li><li>the fire hydrant is on the side of the road  -2.5935</li><li>a fire hydrant on a sidewalk near a street  -2.6334</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000338560.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000532482.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000190664.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000316993.jpg\" ></img></td></tr><tr><td><ol><li>a blue fire hydrant posed on a street corner in a city</li><li>a blue water hydrant on a pavement near the road</li><li>a blue and yellow fire hydrant sitting on the sidewalk next to a quiet street</li><li>a blue and yellow fire hydrant on the side of a road</li><li>a fire hydrant on a sidewalk of a city</li></ol></td><td><ol><li>a green fire hydrant with three yellow concrete barriers around it</li><li>pavement level view of green hydrant near a street corner</li><li>a green fire hydrant surrounded by three yellow poles</li><li>a green fire hydrant sitting between three yellow post</li><li>a green fire hydrant and a bus on the road</li></ol></td><td><ol><li>a fire hydrant that is sitting on the sidewalk</li><li>an orange fire hydrant near the side of the street</li><li>an orange fire hydrant sitting at the side of the street</li><li>a fire hydrant on a sidewalk next to a street</li><li>a UNK hydrant on a side walk near a city street</li></ol></td><td><ol><li>a street intersection that has a traffic light and a direction sign on the corner along</li><li>there is a telescope in the middle of a street</li><li>a street sign near a traffic light pole</li><li>that is a picture of an outside region</li><li>a closeup of a telescope next to a street</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000336040.jpg\" ></img></td></tr><tr><td><ol><li>a fire hydrant is painted silver and blue</li><li>two fire hydrants that are by the street</li><li>two silver and blue fire hydrants side on either side of a road</li><li>silver and blue fire hydrants are placed parallel to each other</li><li>a fire hydrant is painted blue and grey</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000176649.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000176649.jpg\" ></img></td></tr><tr><td><ol><li>a close up of a street sign with a sky background  -5.2408</li><li>a street sign that is on a pole  -9.8394</li><li>a street sign with a sticker on it  -10.0734</li><li>a street sign that says UNK and UNK  -11.1867</li><li>a street sign with a sign on it  -12.2090</li><li>a street sign with a sign that reads UNK  -13.1063</li><li>a sign that says UNK and UNK UNK  -13.5002</li><li>a street sign with a sign that says UNK  -13.5340</li><li>a street sign with a sticker of a man on it  -14.0173</li><li>a sign that says UNK UNK and a street  -15.6800</li></ol></td><td><ol><li>a sign that says UNK UNK and a street  -1.8236</li><li>a street sign with a sticker of a man on it  -2.0566</li><li>a sign that says UNK and UNK UNK  -2.0826</li><li>a close up of a street sign with a sky background  -2.1726</li><li>a street sign with a sticker on it  -2.3357</li><li>a street sign with a sign that says UNK  -2.3788</li><li>a street sign with a sign that reads UNK  -2.3977</li><li>a street sign that is on a pole  -2.7060</li><li>a street sign with a sign on it  -2.7338</li><li>a street sign that says UNK and UNK  -2.8123</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000136411.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000112207.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000037458.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000247547.jpg\" ></img></td></tr><tr><td><ol><li>a stop sign with graffiti on the UNK block of UNK street</li><li>a close up of a vandalized stop sign on a pole</li><li>a stop sign and street sign attached to a pole at an intersection</li><li>a stop sign on UNK street has graffiti</li><li>stop sign with intended UNK written in below it</li></ol></td><td><ol><li>a sign prohibiting bicycle parking with UNK of towing</li><li>a sign indicating that bicycle parking is not allowed</li><li>a red and white street sign stating no bicycle parking</li><li>a picture of a no bicycle parking sign</li><li>a street sign that tells UNK not to park</li></ol></td><td><ol><li>a black and white street sign that reads end bird</li><li>looking up at a street sign that reads end bird</li><li>sign on a street pole saying end bird</li><li>a street sign stands under some power lines</li><li>a sign on a post that reads end bird on it</li></ol></td><td><ol><li>a red stop sign with the word them under it</li><li>a one way sign is attached to a stop sign</li><li>a stop sign with the word stop them on it below a one way</li><li>a stop sign vandalized to read stop them</li><li>a street stop sign with a one way sign attached on top</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000389190.jpg\" ></img></td></tr><tr><td><ol><li>a red sign warning people about pedestrians UNK hit by a crossing guard</li><li>a round red danger railroad crossing sign with a red umbrella in the background</li><li>a warning sign about danger at a railroad crossing</li><li>a sign show that there is danger ahead</li><li>a red danger sign with a person on it</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "for i in range(10):\n",
    "    caps = this_captions[i]\n",
    "#     this_s0_scores = this_log_probs[i]\n",
    "    seq = this_seq[i]\n",
    "    neighbor_batch = get_neighbor_batch(this_feats[i].numpy(), K, include_self=True, self_ix=this_ixs[i])\n",
    "    \n",
    "    # num_sampled_captions x (1+K)\n",
    "    cp_scores = cross_product_scores(\n",
    "        neighbor_batch['fc_feats'].cuda(),\n",
    "        neighbor_batch['att_feats'].cuda(), \n",
    "        neighbor_batch['att_masks'].cuda(),\n",
    "        this_seq[i].cuda()\n",
    "    )\n",
    "    this_s0_scores = cp_scores[:,0].detach().cpu().tolist()\n",
    "    \n",
    "    l1_scores = cp_scores.log_softmax(1)\n",
    "    s1_scores = l1_scores.log_softmax(0)\n",
    "    \n",
    "    this_s1_scores = s1_scores[:,0].detach().cpu().tolist()\n",
    "    \n",
    "    def make_strings(lps, caps):\n",
    "        scored_caps = sorted(list(zip(lps, caps)), reverse=True)\n",
    "        deduped_caps = [next(g) for k, g in groupby(scored_caps, lambda t: t[1])]\n",
    "        cap_strings = [\"{}  {:.4f}\".format(cap, lp) for lp, cap in deduped_caps]\n",
    "        return cap_strings\n",
    "    \n",
    "    display_images([this_paths[i], this_paths[i]], \n",
    "                   captions=[\n",
    "                       make_strings(this_s0_scores, caps)[:10],\n",
    "                       make_strings(this_s1_scores, caps)[:10],\n",
    "                   ])\n",
    "    display_neighbors(data['fc_feats'][i].numpy(), k=min(K, 12), num_per_row=4)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
