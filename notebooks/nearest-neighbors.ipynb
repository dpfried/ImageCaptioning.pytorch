{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dfried/projects/ImageCaptioning.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import captioning.utils.opts as opts\n",
    "import captioning.utils.misc as utils\n",
    "import captioning.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioning.data.dataloader import DataLoader\n",
    "from captioning.data.dataloaderraw import DataLoaderRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "opts.add_eval_options(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=0, beam_size=1, block_trigrams=0, coco_json='', decoding_constraint=0, diversity_lambda=0.5, dump_images=1, dump_json=1, dump_path=0, group_size=1, id='', image_folder='', image_root='', input_att_dir='', input_box_dir='', input_fc_dir='', input_json='', input_label_h5='', language_eval=0, length_penalty='', max_length=20, num_images=-1, remove_bad_endings=0, sample_method='greedy', split='test', suppress_UNK=1, temperature=1.0, verbose_beam=1, verbose_loss=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = 'models/updown/model-best.pth'\n",
    "infos_fname = 'models/updown/infos_tds-best.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab', 'opt', 'best_val_score', 'iter', 'iterators', 'epoch', 'split_ix'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(infos_fname, 'rb') as f:\n",
    "    infos = utils.pickle_load(f)\n",
    "infos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.vocab = infos['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.setup(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del opt.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpDownModel(\n",
       "  (embed): Sequential(\n",
       "    (0): Embedding(9488, 1000)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc_embed): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (att_embed): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (logit): Linear(in_features=1000, out_features=9488, bias=True)\n",
       "  (ctx2att): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (core): UpDownCore(\n",
       "    (att_lstm): LSTMCell(3000, 1000)\n",
       "    (lang_lstm): LSTMCell(2000, 1000)\n",
       "    (attention): Attention(\n",
       "      (h2att): Linear(in_features=1000, out_features=512, bias=True)\n",
       "      (alpha_net): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname, map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = ['input_fc_dir', 'input_att_dir', 'input_box_dir', 'input_label_h5', 'input_json', 'batch_size', 'id']\n",
    "ignore = ['start_from']\n",
    "\n",
    "for k in vars(infos['opt']).keys():\n",
    "    if k in replace:\n",
    "        setattr(opt, k, getattr(opt, k) or getattr(infos['opt'], k, ''))\n",
    "    elif k not in ignore:\n",
    "        if not k in vars(opt):\n",
    "            vars(opt).update({k: vars(infos['opt'])[k]}) # copy over options from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9487\n",
      "DataLoader loading h5 file:  data/cocobu_fc data/cocobu_att data/cocotalk_box data/cocotalk_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(opt, shuffle_override=False, wrap_override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = loader.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11329"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGENERATE:\n",
    "    ixs = []\n",
    "    ids = []\n",
    "    file_paths = []\n",
    "    features = []\n",
    "    captions = []\n",
    "\n",
    "    split = 'train'\n",
    "\n",
    "    loader.reset_iterator(split)\n",
    "\n",
    "    for batch_ix in tqdm.trange(len(loader.loaders[split])):\n",
    "        data = loader.get_batch(split)\n",
    "        infos = data['infos']\n",
    "        ixs.extend([d['ix'] for d in infos])\n",
    "        ids.extend([d['id'] for d in infos])\n",
    "        file_paths.extend([d['file_path'] for d in infos])\n",
    "        features.append(data['fc_feats'])\n",
    "\n",
    "        batch_captions = []\n",
    "        for batch_labels in data['labels']:\n",
    "            instance_captions = []\n",
    "            for img_labels in batch_labels:\n",
    "                caption = [vocab[str(ix.item())] for ix in img_labels if ix != 0]\n",
    "                instance_captions.append(caption)\n",
    "            batch_captions.append(instance_captions)\n",
    "        captions.extend(batch_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = torch.cat(features, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/cocobu_fc/all_{}.pkl'.format(split)\n",
    "if REGENERATE:\n",
    "    with open(fname, 'wb') as f:\n",
    "        d = {\n",
    "            'ixs': ixs,\n",
    "            'ids': ids,\n",
    "            'file_paths': file_paths,\n",
    "            'features': features_array,\n",
    "            'captions': captions,\n",
    "        }\n",
    "        pickle.dump(d, f)\n",
    "else:\n",
    "    with open(fname, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    ixs, ids, file_paths, features_array, captions = d['ixs'], d['ids'], d['file_paths'], d['features'], d['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tag(tag, inner):\n",
    "    return f'<{tag}>{inner}</{tag}>'\n",
    "\n",
    "def image_html(image_path, width=300, border=False):\n",
    "    if border:\n",
    "        style = ' style=\"border: 5px solid #0FF\" '\n",
    "    else:\n",
    "        style = ''\n",
    "    return f'<img width={width} src=\"{image_path}\" {style}></img>'\n",
    "\n",
    "def captions_html(captions):\n",
    "    #return wrap_tag('p', '<br>'.join(' '.join(cap) for cap in captions))\n",
    "    return wrap_tag('ol', ''.join(wrap_tag('li', ' '.join(cap)) for cap in captions))\n",
    "\n",
    "def images_html(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    rows = []\n",
    "    for ix in range(0, len(image_paths), num_per_row):\n",
    "        items = [wrap_tag('td', image_html(image_paths[image_ix], width=width, border=image_ix == target)) \n",
    "                 for image_ix in range(ix, ix+num_per_row) if image_ix < len(image_paths)]\n",
    "        rows.append(wrap_tag('tr', ''.join(items)))\n",
    "        if captions is not None:\n",
    "            cap_html = [\n",
    "                wrap_tag('td', captions_html(captions[image_ix]))\n",
    "                for image_ix in range(ix, ix+num_per_row)\n",
    "                if image_ix < len(image_paths)\n",
    "            ]\n",
    "            rows.append(wrap_tag('tr', ''.join(cap_html)))\n",
    "    return wrap_tag('table', ''.join(rows))\n",
    "\n",
    "def display_images(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    display(HTML(images_html(image_paths, width=width, num_per_row=num_per_row, target=target, captions=captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_neighbors(features, k=5, num_per_row=5):\n",
    "    D, I = index.search(features.flatten()[None], k)\n",
    "    paths_k = [file_paths[ix] for ix in I.flatten()]\n",
    "    captions_k = [captions[ix] for ix in I.flatten()]\n",
    "    display_images(paths_k, captions=captions_k, num_per_row=num_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000318219.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000554625.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000445810.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000407121.jpg\" ></img></td></tr><tr><td><ol><li>a young boy standing in front of a computer keyboard</li><li>a little boy wearing headphones and looking at a computer monitor</li><li>he is listening intently to the computer at school</li><li>a young boy stares up at the computer monitor</li><li>a young kid with head phones on using a computer</li></ol></td><td><ol><li>a boy wearing headphones using one computer in a long row of computers</li><li>a little boy with earphones on listening to something</li><li>a group of people sitting at desk using computers</li><li>children sitting at computer stations on a long table</li><li>a small child wearing headphones plays on the computer</li></ol></td><td><ol><li>some small children playing on laptop games excited</li><li>two children work at a desk on laptop computers</li><li>two young ladies work on laptops on a white counter top</li><li>a little girl sitting in front of a laptop computer</li><li>two girls who are sitting in front of laptops</li></ol></td><td><ol><li>a man with glasses sitting at a desktop computer</li><li>two men at a computer playing game with headphones on</li><li>two men are wearing headphones and playing a computer game</li><li>two men are in the dark by a laptop computer</li><li>dark haired man playing a video game on computer</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000412657.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000356922.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000156471.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000467931.jpg\" ></img></td></tr><tr><td><ol><li>a man with a hoodie and headphones on in front of a computer</li><li>a person sitting in front of a laptop computer wearing glasses</li><li>a man with headphones sitting at a desk looking at a computer</li><li>a young man in a red sweatshirt is on the computer</li><li>a man is sitting at the computer desk with a laptop on it</li></ol></td><td><ol><li>people at a work bench table with laptops and other electronic equipment</li><li>two people on computers amongst a table full of debris</li><li>three people are working on two laptops</li><li>hands are at work at a table repairing laptop computers</li><li>a group of people sitting around a pair of laptops</li></ol></td><td><ol><li>a man holding a smart phone while standing next to a credit card reader</li><li>a man looking at something in his hands</li><li>a young man is at a workstation with a phone</li><li>an image of man that is looking at his cellphone</li><li>a young man is using a cell phone near electronics</li></ol></td><td><ol><li>two men sitting around a laptop looking at the screen</li><li>a man at a laptop with another looking on at his screen</li><li>two men stare intently at a computer screen while one works at the keyboard</li><li>two men at a desk working with a laptop computer</li><li>two people looking at a laptop on a desk</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_neighbors(features_array[1], k=8, num_per_row=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.reset_iterator('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.get_batch('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, seq_logprobs = model(data['fc_feats'], data['att_feats'], data['att_masks'], opt=vars(opt), mode='sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_paths = [d['file_path'] for d in data['infos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_pred_captions = []\n",
    "for sent in seq:\n",
    "    this_pred_captions.append([\n",
    "        [model.vocab.get(str(ix.item()), 'IX_{}'.format(ix.item())) for ix in sent if ix.item() != 0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000184613.jpg\" ></img></td></tr><tr><td><ol><li>a woman is holding a umbrella in a field</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000250804.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000433662.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000020966.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000063043.jpg\" ></img></td></tr><tr><td><ol><li>bulls dogs and people all share the same street</li><li>a black and white cow standing in a market</li><li>two cows and two small dogs hang out in front a street market</li><li>two cows and two dogs and people at an open air market</li><li>two dogs and two cows are amongst people in a market</li></ol></td><td><ol><li>two people standing next to two huge elephants</li><li>two elephants standing near two people with mountains background</li><li>two elephants in the foreground and people in a dirt field</li><li>3 saddled elephants and people in a dirt field</li><li>there are two people standing near two elephants</li></ol></td><td><ol><li>a herd of bulls walking through a town guided by men</li><li>a herd of cattle is pushed through a street past people and vendors</li><li>cows walking in a row down an indian street</li><li>a herd of cattle being led down a dirt road</li><li>a bunch of cows are standing in a pin</li></ol></td><td><ol><li>a huge crowd of people gathered around small tents and livestock</li><li>a very large festival in a rural country with man leading cattle</li><li>a crowd of people standing around a herd of cows</li><li>a man pulling two cows by ropes with a lot of people gathered together</li><li>UNK and animals standing around a campground near a city</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000403013.jpg\" ></img></td></tr><tr><td><ol><li>a kitchen with a stove and a sink</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000048910.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000266273.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000229962.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000332202.jpg\" ></img></td></tr><tr><td><ol><li>a kitchen with a sink a shelf and a chair</li><li>an all white kitchen with a sink and stove</li><li>a kitchen with a shelf cabinets a sink and stove</li><li>a home kitchen with a door leading to the living room</li><li>a narrow kitchen with beams in the ceiling leads to a family room</li></ol></td><td><ol><li>a toddler in a kitchen trying to use a vacuum cleaner</li><li>a mom and a kid in a green kitchen</li><li>a toddler standing around in a kitchen with his mom at the door</li><li>a small boy standing in a kitchen next to a counter</li><li>a mother and baby in the kitchen next to a cabinet and oven</li></ol></td><td><ol><li>a woman standing next to a kitchen sink</li><li>w woman is at the sink in a clean kitchen</li><li>a woman is standing in a kitchen next to the sink</li><li>bright kitchen with woman doing something at the sink</li><li>a person in a very big kitchen by the sink</li></ol></td><td><ol><li>a kitchen filled with appliances and dishes on counters</li><li>a somewhat disorganized looking kitchen with old wooden flooring</li><li>some of the cabinets in the kitchen were left open</li><li>a kitchen has white cabinets and a wood floor</li><li>an l shaped white kitchen with green marble counter tops gets plenty of natural light from</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000562150.jpg\" ></img></td></tr><tr><td><ol><li>a woman holding a cat in her arms</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000277050.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000184937.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000478675.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000361217.jpg\" ></img></td></tr><tr><td><ol><li>a young lady is looking on while petting an owl</li><li>animal keeper with owl perched on one arm</li><li>a woman holding and petting an owl on a lush green field</li><li>a woman holding an owl on her arm</li><li>a woman outside holding a small owl on her arm</li></ol></td><td><ol><li>a woman is petting a cat and drinking coffee</li><li>a woman sitting on steps petting a cat</li><li>a woman sitting and holding a cup while petting a cat</li><li>a woman holding a beverage petting a cat</li><li>a woman sitting on steps outside is petting a cat</li></ol></td><td><ol><li>a guy holds a cat who is wearing antlers</li><li>man holding a cat that is wearing a costume</li><li>a man is holding a cat in his hands</li><li>a man holding onto a cat and a plant</li><li>there is a man holding a cat but theres something on the cat</li></ol></td><td><ol><li>a couple of pretty young ladies holding kittens</li><li>girls holding kittens while they are being pet</li><li>two women hold cats while others pet the cats</li><li>girls holding kittens in an outdoor space</li><li>girls holding up small white and grey kittens together</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000360772.jpg\" ></img></td></tr><tr><td><ol><li>a bathroom with a toilet and a toilet paper roll</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000012323.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000386560.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000567179.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000208754.jpg\" ></img></td></tr><tr><td><ol><li>a toilet and two rolls of toilet paper in a small room with ledge and window</li><li>a toilet inside a small bathroom being UNK</li><li>a bath room with a toilet and two rolls of toilet paper</li><li>the bathroom with a toilet having an unfinished wall behind it</li><li>a picture of a toilet seat with the lid up in an unfinished bathroom</li></ol></td><td><ol><li>a bathroom with a toilet a roll of UNK and a counter with a missing cabinet</li><li>a bathroom with a toilet and sink sitting on a tiled floor</li><li>a toilet in a bathroom that is being remodeled</li><li>a bathroom toilet with a mirror above and toilet roll</li><li>a single roll of toilet paper sits on top of the toilet tank</li></ol></td><td><ol><li>a bathroom with a toilet sink and toilet paper roll</li><li>a toilet a cabinet a sink a mirror and tan tiles</li><li>a small bathroom with a sink and a toilet</li><li>a bathroom displays a toilet and a sink</li><li>a bathroom with a toilet bowl and sink</li></ol></td><td><ol><li>a young man laying on top of a white toilet seat near a sink</li><li>a toddler playing near the toilet in a bathroom</li><li>a toddler is leaning against a closed toilet</li><li>a little boy playing with a toilet</li><li>a bath room with a toilet and a child on the toilet</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000340559.jpg\" ></img></td></tr><tr><td><ol><li>a large kitchen with a sink and a sink</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000432201.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000042568.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000229311.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000078266.jpg\" ></img></td></tr><tr><td><ol><li>clean stainless steel kitchen with large cabinets and counter</li><li>a small spotless and clean stainless steel kitchen</li><li>a stainless UNK kitchen with sinks and lots of storage</li><li>a restaurant kitchen stocked with stainless steel appliances</li><li>a kitchen with a sink ovens and dishes in it</li></ol></td><td><ol><li>large shower sectional of a bathroom in a brown and white photograph</li><li>a open shower stall that has a robe next to it</li><li>a bathroom with a stand up shower and tub</li><li>a bathroom with a tub next to a fancy shower stall</li><li>a walk in shower sitting next to a bath tub</li></ol></td><td><ol><li>a stainless steel kitchen sink on a black granite countertop</li><li>a kitchen with a sink on a counter top</li><li>a kitchen with a sink near a window</li><li>a kitchen area with a large stainless steel sink</li><li>an empty kitchen sink underneath a window on the counter</li></ol></td><td><ol><li>kitchen utensils and appliances have been left unattended</li><li>a work room that looks like a dry UNK</li><li>the cramped interior of a passenger ships kitchen</li><li>a room with a bunch of stainless steel items and other accessories</li><li>a clean industrial kitchen with no one in it</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000321107.jpg\" ></img></td></tr><tr><td><ol><li>a woman riding a bike down a street next to a man</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000329755.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000493846.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000016961.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000087920.jpg\" ></img></td></tr><tr><td><ol><li>a man on a bicycle stopped at an intersection</li><li>a man with an orange vest on a bicycle is at an intersection</li><li>a man in an orange vest riding a bike</li><li>man on bike smiling at camera while sitting in intersection</li><li>a man is riding his bike that has packages strapped to the back</li></ol></td><td><ol><li>a man riding a bike down a street past a young man</li><li>an out of shape man rides a bicycle</li><li>an older man is riding his bike down the road</li><li>a person riding a bike on a city street</li><li>an older man riding a bike on a street</li></ol></td><td><ol><li>a guy riding a skateboard on the road with a long pole</li><li>a man long boarding with a stick to propel him</li><li>a man with a stick pushing himself by skateboard</li><li>guy riding skateboard with a long stick while others ride bikes around him</li><li>adult male pushing himself on a skateboard with a stick</li></ol></td><td><ol><li>a lady is riding a bicycle while talking on a cell phone</li><li>a woman rides her bike and talks on the phone</li><li>woman on cell phone bicycling down the street</li><li>a woman on a bike while on a cell phone</li><li>a woman riding a bicycle making a call on the road near a building with cars</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000129001.jpg\" ></img></td></tr><tr><td><ol><li>a bathroom with a shower and a sink</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000037126.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000187972.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000383606.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000134715.jpg\" ></img></td></tr><tr><td><ol><li>camera flash reflected in a mirror in a small tiled bathroom</li><li>flash from camera glares in the mirror above the toilet</li><li>a very small rest room with a toilet and sink</li><li>a bath room with a toilet a sink and a mirror</li><li>a person take a picture of a bathroom</li></ol></td><td><ol><li>a bathroom that has its lights turned on</li><li>a simple modern bathroom is displayed in the dim light</li><li>a bathroom has a window and a glass door</li><li>a modern bathroom offers visitors a touch of privacy during toiletry</li><li>a bathroom that is very white with a light on</li></ol></td><td><ol><li>a blue corner sink with a man reflected in the above mirrors</li><li>a bathroom with a sink and a mirror</li><li>a person taking a picture inside a bathroom with a blue sink and a brown door</li><li>compact hotel bathroom with corner basin and mirrors</li><li>a man holding a camera reflected in a bathroom mirror that sits above a sink and</li></ol></td><td><ol><li>a large mirror above a sink in a bathroom</li><li>a view of a bathroom that looks very elegant</li><li>a bathroom with vanity toilet and tub is decorated in UNK and browns</li><li>a bathroom showing sink toilet and shower</li><li>a shower that has a bar in it</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000556616.jpg\" ></img></td></tr><tr><td><ol><li>a parking meter with a parking meter on the side of it</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000253528.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000539767.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000135410.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000068789.jpg\" ></img></td></tr><tr><td><ol><li>a row of parking meters sitting next to parked cars</li><li>a row of parking meters along a road</li><li>a sidewalk with many parking meters going all the way down the street</li><li>a boy in red is walking past change meters</li><li>a city street lined with parking meters and parked cars</li></ol></td><td><ol><li>a red bike parked in a parking lot next to buildings</li><li>a view of a parked motorcycle from behind</li><li>a close up of a red parked motorcycle on pavement</li><li>a view of a motorcycle from its back end</li><li>the large motorcycle has two cup holders on it</li></ol></td><td><ol><li>a bike leaning up to a parking meter</li><li>a coin meter that is next to a ladder with balls on it</li><li>a parking meter is on the street in front of a building</li><li>a stack of balls that is next to a parking meter</li><li>a parking meter next to a ladder with bowling balls on each UNK</li></ol></td><td><ol><li>a bunch of cars are passing by a couple meters</li><li>a street wet from rain and crowded with cars</li><li>some parking meters sitting on the side of the road</li><li>a wet street with moving cars parked cars and parking meters</li><li>there are many cars and UNK on this street</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000472621.jpg\" ></img></td></tr><tr><td><ol><li>a bathroom with a tub and a sink</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"train2014/COCO_train2014_000000515287.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000188343.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000343670.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000474732.jpg\" ></img></td></tr><tr><td><ol><li>a white bathroom with a tub sink and mirror</li><li>a bathroom has a sink and bathtub and open door</li><li>a bathroom scene with photo taken above the bath tub</li><li>a bathroom with a sink and mirror and a bathtub</li><li>a bathroom has a sink and a tub</li></ol></td><td><ol><li>there is a hairdryer mounted on the wall next to the double sink vanity</li><li>the bathroom has two sinks a large mirror and a hair dryer</li><li>a bathroom counter topped with personal care items</li><li>a modern residential bathroom with walk in shower</li><li>a bathroom with double marble sinks is equipped with a wall hanging blow dryer</li></ol></td><td><ol><li>a bathroom with his and her sinks under a large mirror</li><li>two round sinks are in a wooden counter top next to a large mirror</li><li>a shiny white bathroom with a wood decorated counter</li><li>a bathroom with two sinks and a mirror</li><li>the double sink in the bathroom is nice and clean</li></ol></td><td><ol><li>a bathroom with a sink a mirror and a UNK</li><li>a bathroom with a sink mirror and towel rack</li><li>a white sink under a mirror and red walls</li><li>a bathroom with red walls a sink and a table</li><li>a bathroom painted red with a sink and wall light</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000364521.jpg\" ></img></td></tr><tr><td><ol><li>a yellow fire hydrant with a red and yellow UNK</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000358345.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000147196.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000086071.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000424739.jpg\" ></img></td></tr><tr><td><ol><li>a water hydrant on the side of the road</li><li>a dog s diaper is attached to a fire hydrant</li><li>a baby bib hangs on a yellow fire hydrant</li><li>yellow fire hydrant with a babys garment hanging on it</li><li>a fire hydrant with a piece of clothing draped over one edge</li></ol></td><td><ol><li>a green and black fire hydrant with a red top in the dry grass</li><li>a black and white fire hydrant topped with a red ball</li><li>a black and white fire hydrant with a red foam clown nose on top</li><li>a painted fire hydrant sitting in dead grass</li><li>black and white fire hydrant surrounded by dead grass</li></ol></td><td><ol><li>a yellow fire hydrant is sitting on a gravel road</li><li>a yellow fire hydrant sitting next to a red fire hydrant</li><li>the fire hydrant is being capped off by something different</li><li>a yellow fire hydrant in a gravel area</li><li>a fire hydrant is attached to a separated shut off apparatus</li></ol></td><td><ol><li>a tree trunk having some UNK tools and containers resting on top</li><li>camping equipment including a portable stove a UNK and a multi tool</li><li>a large glass jar sitting on top of a wooden table next to construction items</li><li>a knife a lighter and several other UNK items sit on the ground</li><li>items placed on a tree stump for making a cooking stove</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ix in range(10):\n",
    "    display_images(this_paths[ix:ix+1], captions=this_pred_captions[ix:ix+1])\n",
    "    display_neighbors(data['fc_feats'][ix].numpy(), k=4, num_per_row=4)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IX_0 a long hot dog on a plate on a table IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a very long hot dog on a plastic plate IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a foot long hot dog on top of two buns IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 long hot dog using two buns on paper plate IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a foot long hotdog on two regular buns on a styrofoam plate IX_0 IX_0 IX_0 IX_0 IX_0\n"
     ]
    }
   ],
   "source": [
    "for sent in d['labels'][0]:\n",
    "    print(' '.join(loader.dataset.get_vocab().get(str(ix.item()), 'IX_{}'.format(ix.item())) for ix in sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
