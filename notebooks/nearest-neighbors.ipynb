{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dfried/projects/ImageCaptioning.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import captioning.utils.opts as opts\n",
    "import captioning.utils.misc as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioning.data.dataloader import DataLoader\n",
    "from captioning.data.dataloaderraw import DataLoaderRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "opts.add_eval_options(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=0, beam_size=1, block_trigrams=0, coco_json='', decoding_constraint=0, diversity_lambda=0.5, dump_images=1, dump_json=1, dump_path=0, group_size=1, id='', image_folder='', image_root='', input_att_dir='', input_box_dir='', input_fc_dir='', input_json='', input_label_h5='', language_eval=0, length_penalty='', max_length=20, num_images=-1, remove_bad_endings=0, sample_method='greedy', split='test', suppress_UNK=1, temperature=1.0, verbose_beam=1, verbose_loss=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab', 'opt', 'best_val_score', 'iter', 'iterators', 'epoch', 'split_ix'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('models/updown/infos_tds-best.pkl', 'rb') as f:\n",
    "    infos = utils.pickle_load(f)\n",
    "infos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = ['input_fc_dir', 'input_att_dir', 'input_box_dir', 'input_label_h5', 'input_json', 'batch_size', 'id']\n",
    "ignore = ['start_from']\n",
    "\n",
    "for k in vars(infos['opt']).keys():\n",
    "    if k in replace:\n",
    "        setattr(opt, k, getattr(opt, k) or getattr(infos['opt'], k, ''))\n",
    "    elif k not in ignore:\n",
    "        if not k in vars(opt):\n",
    "            vars(opt).update({k: vars(infos['opt'])[k]}) # copy over options from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9487\n",
      "DataLoader loading h5 file:  data/cocobu_fc data/cocobu_att data/cocotalk_box data/cocotalk_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(opt, shuffle_override=False, wrap_override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = loader.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11329"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11329/11329 [03:50<00:00, 49.12it/s]\n"
     ]
    }
   ],
   "source": [
    "if REGENERATE:\n",
    "    ixs = []\n",
    "    ids = []\n",
    "    file_paths = []\n",
    "    features = []\n",
    "    captions = []\n",
    "\n",
    "    split = 'train'\n",
    "\n",
    "    loader.reset_iterator(split)\n",
    "\n",
    "    for batch_ix in tqdm.trange(len(loader.loaders[split])):\n",
    "        data = loader.get_batch(split)\n",
    "        infos = data['infos']\n",
    "        ixs.extend([d['ix'] for d in infos])\n",
    "        ids.extend([d['id'] for d in infos])\n",
    "        file_paths.extend([d['file_path'] for d in infos])\n",
    "        features.append(data['fc_feats'])\n",
    "\n",
    "        batch_captions = []\n",
    "        for batch_labels in data['labels']:\n",
    "            instance_captions = []\n",
    "            for img_labels in batch_labels:\n",
    "                caption = [vocab[str(ix.item())] for ix in img_labels if ix != 0]\n",
    "                instance_captions.append(caption)\n",
    "            batch_captions.append(instance_captions)\n",
    "        captions.extend(batch_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = torch.cat(features, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/cocobu_fc/all_{}.pkl'.format(split)\n",
    "if REGENERATE:\n",
    "    with open(fname, 'wb') as f:\n",
    "        d = {\n",
    "            'ixs': ixs,\n",
    "            'ids': ids,\n",
    "            'file_paths': file_paths,\n",
    "            'features': features_array,\n",
    "            'captions': captions,\n",
    "        }\n",
    "        pickle.dump(d, f)\n",
    "else:\n",
    "    with open(fname, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    ixs, ids, file_paths, features_array, captions = d['ixs'], d['ids'], d['file_paths'], d['features'], d['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tag(tag, inner):\n",
    "    return f'<{tag}>{inner}</{tag}>'\n",
    "\n",
    "def image_html(image_path, width=300, border=False):\n",
    "    if border:\n",
    "        style = ' style=\"border: 5px solid #0FF\" '\n",
    "    else:\n",
    "        style = ''\n",
    "    return f'<img width={width} src=\"{image_path}\" {style}></img>'\n",
    "\n",
    "def captions_html(captions):\n",
    "    #return wrap_tag('p', '<br>'.join(' '.join(cap) for cap in captions))\n",
    "    return wrap_tag('ol', ''.join(wrap_tag('li', ' '.join(cap)) for cap in captions))\n",
    "\n",
    "def images_html(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    rows = []\n",
    "    for ix in range(0, len(image_paths), num_per_row):\n",
    "        items = [wrap_tag('td', image_html(image_paths[image_ix], width=width, border=image_ix == target)) \n",
    "                 for image_ix in range(ix, ix+num_per_row) if image_ix < len(image_paths)]\n",
    "        rows.append(wrap_tag('tr', ''.join(items)))\n",
    "        if captions is not None:\n",
    "            cap_html = [\n",
    "                wrap_tag('td', captions_html(captions[image_ix]))\n",
    "                for image_ix in range(ix, ix+num_per_row)\n",
    "                if image_ix < len(image_paths)\n",
    "            ]\n",
    "            rows.append(wrap_tag('tr', ''.join(cap_html)))\n",
    "    return wrap_tag('table', ''.join(rows))\n",
    "\n",
    "def display_images(image_paths, width=300, num_per_row=5, target=None, captions=None):\n",
    "    display(HTML(images_html(image_paths, width=width, num_per_row=num_per_row, target=target, captions=captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_neighbors(ix, k=5, num_per_row=5):\n",
    "    D, I = index.search(features_array[ix][None], k)\n",
    "    paths_k = [file_paths[ix] for ix in I.flatten()]\n",
    "    captions_k = [captions[ix] for ix in I.flatten()]\n",
    "    display_images(paths_k, captions=captions_k, num_per_row=num_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img width=300 src=\"val2014/COCO_val2014_000000318219.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000554625.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000445810.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000407121.jpg\" ></img></td></tr><tr><td><ol><li>a young boy standing in front of a computer keyboard</li><li>a little boy wearing headphones and looking at a computer monitor</li><li>he is listening intently to the computer at school</li><li>a young boy stares up at the computer monitor</li><li>a young kid with head phones on using a computer</li></ol></td><td><ol><li>a boy wearing headphones using one computer in a long row of computers</li><li>a little boy with earphones on listening to something</li><li>a group of people sitting at desk using computers</li><li>children sitting at computer stations on a long table</li><li>a small child wearing headphones plays on the computer</li></ol></td><td><ol><li>some small children playing on laptop games excited</li><li>two children work at a desk on laptop computers</li><li>two young ladies work on laptops on a white counter top</li><li>a little girl sitting in front of a laptop computer</li><li>two girls who are sitting in front of laptops</li></ol></td><td><ol><li>a man with glasses sitting at a desktop computer</li><li>two men at a computer playing game with headphones on</li><li>two men are wearing headphones and playing a computer game</li><li>two men are in the dark by a laptop computer</li><li>dark haired man playing a video game on computer</li></ol></td></tr><tr><td><img width=300 src=\"train2014/COCO_train2014_000000412657.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000356922.jpg\" ></img></td><td><img width=300 src=\"train2014/COCO_train2014_000000156471.jpg\" ></img></td><td><img width=300 src=\"val2014/COCO_val2014_000000467931.jpg\" ></img></td></tr><tr><td><ol><li>a man with a hoodie and headphones on in front of a computer</li><li>a person sitting in front of a laptop computer wearing glasses</li><li>a man with headphones sitting at a desk looking at a computer</li><li>a young man in a red sweatshirt is on the computer</li><li>a man is sitting at the computer desk with a laptop on it</li></ol></td><td><ol><li>people at a work bench table with laptops and other electronic equipment</li><li>two people on computers amongst a table full of debris</li><li>three people are working on two laptops</li><li>hands are at work at a table repairing laptop computers</li><li>a group of people sitting around a pair of laptops</li></ol></td><td><ol><li>a man holding a smart phone while standing next to a credit card reader</li><li>a man looking at something in his hands</li><li>a young man is at a workstation with a phone</li><li>an image of man that is looking at his cellphone</li><li>a young man is using a cell phone near electronics</li></ol></td><td><ol><li>two men sitting around a laptop looking at the screen</li><li>a man at a laptop with another looking on at his screen</li><li>two men stare intently at a computer screen while one works at the keyboard</li><li>two men at a desk working with a laptop computer</li><li>two people looking at a laptop on a desk</li></ol></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_neighbors(1, k=8, num_per_row=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IX_0 a long hot dog on a plate on a table IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a very long hot dog on a plastic plate IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a foot long hot dog on top of two buns IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 long hot dog using two buns on paper plate IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0 IX_0\n",
      "IX_0 a foot long hotdog on two regular buns on a styrofoam plate IX_0 IX_0 IX_0 IX_0 IX_0\n"
     ]
    }
   ],
   "source": [
    "for sent in d['labels'][0]:\n",
    "    print(' '.join(loader.dataset.get_vocab().get(str(ix.item()), 'IX_{}'.format(ix.item())) for ix in sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['fc_feats'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
